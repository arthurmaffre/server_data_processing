{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "04968abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "from transformers import AutoModel\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import os\n",
    "from huggingface_hub import hf_hub_download\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ae1e8c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "repo_id_download = \"Arthurmaffre34/pre-dataset\"\n",
    "repo_id_upload = \"Gill-Hack-25-UdeM/Text_Embedding_Dataset\"\n",
    "\n",
    "sub_batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5952e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset complet: 9516 lignes\n",
      "Dataset réduit : 2 lignes\n"
     ]
    }
   ],
   "source": [
    "def download_dataset(repo_id_download: str, filename: str = \"pre_dataset.parquet\", private: bool = False):\n",
    "    \"\"\"\n",
    "    Download a dataset file from the Hugging Face Hub and load it into a pandas DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        repo_id (str): Hugging Face repo ID (e.g., \"Arthurmaffre34/pre-dataset\").\n",
    "        filename (str): The file name inside the repo (default: \"pre_dataset.parquet\").\n",
    "        private (bool): Set to True if the repo is private (requires HF_TOKEN).\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: The loaded dataset.\n",
    "    \"\"\"\n",
    "    file_path = hf_hub_download(\n",
    "        repo_id=repo_id_download,\n",
    "        filename=filename,\n",
    "        repo_type=\"dataset\",\n",
    "        token=os.getenv(\"HF_TOKEN\") if private else None\n",
    "    )\n",
    "    return pd.read_parquet(file_path)\n",
    "\n",
    "def reduce_dataset(df: pd.DataFrame, frac: float = None, n: int = None, seed: int = 42) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reduce the dataset by sampling either a fraction or a fixed number of rows.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataset.\n",
    "        frac (float, optional): Fraction of the dataset to sample (e.g., 0.01 for 1%).\n",
    "        n (int, optional): Exact number of rows to sample.\n",
    "        seed (int): Random seed for reproducibility (default: 42).\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Reduced dataset.\n",
    "    \"\"\"\n",
    "    if frac is not None:\n",
    "        return df.sample(frac=frac, random_state=seed).reset_index(drop=True)\n",
    "    elif n is not None:\n",
    "        return df.sample(n=n, random_state=seed).reset_index(drop=True)\n",
    "    else:\n",
    "        raise ValueError(\"You must specify either `frac` or `n`.\")\n",
    "\n",
    "df = download_dataset(repo_id_download)\n",
    "\n",
    "df_small = reduce_dataset(df, n = 2)\n",
    "\n",
    "print(f\"Dataset complet: {len(df)} lignes\")\n",
    "print(f\"Dataset réduit : {len(df_small)} lignes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2d9e688",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModel.from_pretrained(\"yiyanghkust/finbert-pretrain\")\n",
    "model.eval().to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"yiyanghkust/finbert-pretrain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d22bfc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import warnings\n",
    "\n",
    "class FinBertEmbeddingDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, model, max_length=512, n_chunks=100, sub_batch_size=100, device=\"cpu\"):\n",
    "        # --- Error checks ---\n",
    "        if max_length > 512:\n",
    "            raise ValueError(f\"❌ max_length={max_length} is not allowed. FinBERT only supports max_length ≤ 512.\")\n",
    "        if sub_batch_size > n_chunks:\n",
    "            raise ValueError(f\"❌ sub_batch_size={sub_batch_size} cannot be greater than n_chunks={n_chunks}.\")\n",
    "\n",
    "        # --- Tokenizer check ---\n",
    "        expected_model_name = \"yiyanghkust/finbert-pretrain\"\n",
    "        if getattr(tokenizer, \"name_or_path\", None) != expected_model_name:\n",
    "            warnings.warn(\n",
    "                f\"⚠️ Tokenizer `{tokenizer.name_or_path}` does not match expected `{expected_model_name}`. \"\n",
    "                \"This may cause mismatches between embeddings and vocabulary.\",\n",
    "                UserWarning\n",
    "            )\n",
    "\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model.to(device)\n",
    "        self.max_length = max_length\n",
    "        self.n_chunks = n_chunks\n",
    "        self.pad_id = tokenizer.pad_token_id\n",
    "        self.sub_batch_size = sub_batch_size\n",
    "        self.device = device\n",
    "        self.model.eval()\n",
    "\n",
    "    def chunk_text(self, text):\n",
    "        tokens = self.tokenizer.encode(text, add_special_tokens=True)\n",
    "        chunks = [tokens[i:i+self.max_length] for i in range(0, len(tokens), self.max_length)]\n",
    "\n",
    "        ids, masks = [], []\n",
    "        for chunk in chunks[:self.n_chunks]:\n",
    "            attn = [1] * len(chunk)\n",
    "            if len(chunk) < self.max_length:\n",
    "                pad_len = self.max_length - len(chunk)\n",
    "                chunk = chunk + [self.pad_id] * pad_len\n",
    "                attn = attn + [0] * pad_len\n",
    "            ids.append(chunk)\n",
    "            masks.append(attn)\n",
    "\n",
    "        while len(ids) < self.n_chunks:\n",
    "            ids.append([self.pad_id] * self.max_length)\n",
    "            masks.append([0] * self.max_length)\n",
    "\n",
    "        return torch.tensor(ids, dtype=torch.long), torch.tensor(masks, dtype=torch.long)\n",
    "\n",
    "    def encode_chunks(self, ids, mask, pbar=None):\n",
    "        outputs = []\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, ids.size(0), self.sub_batch_size):\n",
    "                ids_sub = ids[i:i+self.sub_batch_size].to(self.device)\n",
    "                mask_sub = mask[i:i+self.sub_batch_size].to(self.device)\n",
    "                out = self.model(ids_sub, attention_mask=mask_sub).pooler_output\n",
    "                outputs.append(out.cpu())\n",
    "\n",
    "                if pbar is not None:\n",
    "                    pbar.update(ids_sub.size(0))  # global increment\n",
    "        return torch.cat(outputs, dim=0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        rf_ids, rf_mask = self.chunk_text(str(row[\"rf\"]))\n",
    "        mgmt_ids, mgmt_mask = self.chunk_text(str(row[\"mgmt\"]))\n",
    "\n",
    "        rf_emb = self.encode_chunks(rf_ids, rf_mask, pbar=getattr(self, \"pbar\", None))\n",
    "        mgmt_emb = self.encode_chunks(mgmt_ids, mgmt_mask, pbar=getattr(self, \"pbar\", None))\n",
    "\n",
    "        stacked = torch.stack([rf_emb, mgmt_emb], dim=0)\n",
    "\n",
    "        return {\n",
    "            \"embeddings\": stacked,\n",
    "            \"labels\": torch.tensor(row[\"return\"], dtype=torch.float)\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b00d8734",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding all chunks: 100%|██████████| 400/400 [00:22<00:00, 17.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Embeddings shape: torch.Size([2, 2, 100, 768])\n",
      "✅ Labels shape: torch.Size([2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def prepare_dataset(df, tokenizer, model, sub_batch_size=32, device=\"cpu\", n_chunks=100, batch_size=1):\n",
    "    \"\"\"\n",
    "    Encode a dataset into FinBERT embeddings with progress bar.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataframe with \"rf\", \"mgmt\", and \"return\".\n",
    "        tokenizer: Hugging Face tokenizer (e.g., FinBERT tokenizer).\n",
    "        model: Hugging Face model (e.g., FinBERT model).\n",
    "        sub_batch_size (int): Number of chunks processed at once inside FinBERT.\n",
    "        device (str): \"cpu\" or \"cuda\".\n",
    "        n_chunks (int): Number of chunks per document.\n",
    "        batch_size (int): DataLoader batch size (default=1).\n",
    "    \n",
    "    Returns:\n",
    "        (torch.Tensor, torch.Tensor): embeddings [num_samples, 2, N, hidden_dim], labels [num_samples]\n",
    "    \"\"\"\n",
    "    dataset = FinBertEmbeddingDataset(\n",
    "        df, tokenizer, model,\n",
    "        sub_batch_size=sub_batch_size,\n",
    "        device=device,\n",
    "        n_chunks=n_chunks\n",
    "    )\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    total_chunks = len(dataset) * 2 * dataset.n_chunks  # RF + MGMT\n",
    "    all_embeddings, all_labels = [], []\n",
    "\n",
    "    with tqdm(total=total_chunks, desc=\"Encoding all chunks\") as pbar:\n",
    "        dataset.pbar = pbar\n",
    "        for batch in loader:\n",
    "            all_embeddings.append(batch[\"embeddings\"])\n",
    "            all_labels.append(batch[\"labels\"])\n",
    "        dataset.pbar = None  # cleanup\n",
    "\n",
    "    embeddings = torch.cat(all_embeddings)   # [num_samples, 2, N, hidden_dim]\n",
    "    labels = torch.cat(all_labels)           # [num_samples]\n",
    "    return embeddings, labels\n",
    "\n",
    "embeddings, labels = prepare_dataset(\n",
    "    df=df_small,\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    sub_batch_size=16,\n",
    "    device=device,\n",
    "    n_chunks=100,\n",
    "    batch_size=1\n",
    ")\n",
    "\n",
    "print(\"✅ Embeddings shape:\", embeddings.shape)\n",
    "print(\"✅ Labels shape:\", labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8cf31ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Vérification dataset ===\n",
      "Embeddings shape : torch.Size([2, 2, 100, 768])\n",
      "Labels shape     : torch.Size([2])\n",
      "Type embeddings  : torch.float32\n",
      "Type labels      : torch.float32\n",
      "Exemple label    : 0.06242643669247627\n",
      "Exemple batch    : torch.Size([2, 100, 768])\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Vérification dataset ===\")\n",
    "print(\"Embeddings shape :\", embeddings.shape)\n",
    "print(\"Labels shape     :\", labels.shape)\n",
    "print(\"Type embeddings  :\", embeddings.dtype)\n",
    "print(\"Type labels      :\", labels.dtype)\n",
    "print(\"Exemple label    :\", labels[0].item())\n",
    "print(\"Exemple batch    :\", embeddings[0].shape)  # [2, N, hidden_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb21e576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Target max file size: 0.5 GB\n",
      "⚖️  Estimated 600.00 KB per sample\n",
      "➡️  873 samples per shard → 1 shards total\n",
      "✅ Saved finbert_embeddings_part0.pt (1.17 MB, 2 samples)\n",
      "✅ Dataset saved in 1 shards\n"
     ]
    }
   ],
   "source": [
    "def save_sharded_dataset(embeddings, labels, max_file_size_gb=1, prefix=\"finbert_embeddings_part\"):\n",
    "    \"\"\"\n",
    "    Save embeddings + labels into multiple shards based on target file size.\n",
    "\n",
    "    Args:\n",
    "        embeddings (torch.Tensor): Tensor of shape [N, ...].\n",
    "        labels (torch.Tensor): Tensor of shape [N].\n",
    "        max_file_size_gb (int): Maximum shard size in gigabytes (default=1 GB).\n",
    "        prefix (str): Prefix for saved shard files.\n",
    "\n",
    "    Returns:\n",
    "        int: Number of shards created.\n",
    "    \"\"\"\n",
    "    # Estimate bytes per sample\n",
    "    bytes_per_sample = (\n",
    "        embeddings[0].element_size() * embeddings[0].numel() +\n",
    "        labels[0].element_size()\n",
    "    )\n",
    "    shard_size = max(1, int((max_file_size_gb * (1024**3)) // bytes_per_sample))\n",
    "\n",
    "    n_samples = len(embeddings)\n",
    "    n_shards = math.ceil(n_samples / shard_size)\n",
    "\n",
    "    print(f\"📦 Target max file size: {max_file_size_gb} GB\")\n",
    "    print(f\"⚖️  Estimated {bytes_per_sample/1024:.2f} KB per sample\")\n",
    "    print(f\"➡️  {shard_size} samples per shard → {n_shards} shards total\")\n",
    "\n",
    "    for i in range(n_shards):\n",
    "        start = i * shard_size\n",
    "        end = min((i+1) * shard_size, n_samples)\n",
    "        filename = f\"{prefix}{i}.pt\"\n",
    "        torch.save(\n",
    "            {\"embeddings\": embeddings[start:end], \"labels\": labels[start:end]},\n",
    "            filename\n",
    "        )\n",
    "        file_size_mb = os.path.getsize(filename) / (1024**2)\n",
    "        print(f\"✅ Saved {filename} ({file_size_mb:.2f} MB, {end-start} samples)\")\n",
    "\n",
    "    return n_shards\n",
    "\n",
    "# Save shards targeting ~500 MB each\n",
    "n_shards = save_sharded_dataset(embeddings, labels, max_file_size_gb=0.5)\n",
    "\n",
    "print(f\"✅ Dataset saved in {n_shards} shards\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8b8ca07c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b359952b64bb4ec3849751324a727582",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bec1d000211458cbf33454983e46106",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload                         : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eace5e82bc649fca2ef2af6b0d5af93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  finbert_embeddings_part0.pt           : 100%|##########| 1.23MB / 1.23MB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔼 Upload finbert_embeddings_part0.pt vers HF terminé\n"
     ]
    }
   ],
   "source": [
    "def upload_dataset_shards_to_hf(repo_id_upload, n_shards, prefix=\"finbert_embeddings_part\", repo_type=\"dataset\"):\n",
    "    from huggingface_hub import HfApi\n",
    "    api = HfApi()\n",
    "    for i in range(n_shards):\n",
    "        filename = f\"{prefix}{i}.pt\"\n",
    "        api.upload_file(\n",
    "            path_or_fileobj=filename,\n",
    "            path_in_repo=filename,\n",
    "            repo_id=repo_id_upload,\n",
    "            repo_type=repo_type,\n",
    "            create_pr=True\n",
    "        )\n",
    "        print(f\"🔼 Upload {filename} vers HF terminé\")\n",
    "upload_dataset_shards_to_hf(repo_id_upload, n_shards)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ECN6338",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
